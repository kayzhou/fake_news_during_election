{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向量化（vectorize）\n",
    "\n",
    "from my_weapon import *\n",
    "from gensim.models import Word2Vec\n",
    "import word2vecReader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import autograd, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    2.425314e+06\n",
      "mean     1.650435e+01\n",
      "std      5.510423e+00\n",
      "min      1.000000e+00\n",
      "25%      1.300000e+01\n",
      "50%      1.700000e+01\n",
      "75%      2.000000e+01\n",
      "max      1.110000e+02\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 统计最长的序列\n",
    "\n",
    "d = pd.Series([len(line.strip().split(\" \")) for line in open(\"data/train.txt\")])\n",
    "print(d.describe())\n",
    "d.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model = Word2Vec.load(\"model/word2vec.mod\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最长的序列是111个tokens，我决定用40作为每个句子的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update vectorize\n",
    "# 需要经过tweet_process.py的处理\n",
    "\n",
    "def vectorize(line):\n",
    "    v = np.zeros(40 * 400).reshape(40, 400)\n",
    "    words = line.strip().split(\" \")\n",
    "    _index = 0\n",
    "    for w in words:\n",
    "        if _index >= 40:\n",
    "            break\n",
    "        if w in wv_model.wv:\n",
    "            v[_index] = wv_model.wv[w]\n",
    "            _index += 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.08587575,  0.41580471, -2.2741425 , ..., -0.0125636 ,\n",
       "         2.06250668,  0.08311198],\n",
       "       [-0.81667441,  0.94962615, -1.45155382, ...,  0.22324868,\n",
       "         1.16281199, -0.71577537],\n",
       "       [-0.68626755,  0.78144592, -2.61305523, ...,  0.71367294,\n",
       "         0.76811117,  2.10258842],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize(\"what are you doing ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们看看另外一个已经训练好的在大的数据集上的word2vec model\n",
    "\n",
    "location: /media/alex/data/word2vec_twitter_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README\tword2vecReader.py  word2vecReaderUtils.py  word2vec_twitter_model.bin\n"
     ]
    }
   ],
   "source": [
    "!ls /media/alex/data/word2vec_twitter_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This zip contains a word2vec model trained on Twitter data as described in:\n",
      "\n",
      "Godin, F., Vandersmissen, B., De Neve, W., & Van de Walle, R. (2015).\n",
      "Multimedia Lab @ ACL W-NUT NER shared task: Named entity recognition for Twitter microposts using distributed word representations.\n",
      "Workshop on Noisy User-generated Text, ACL 2015.\n",
      "\n",
      "Please cite the paper if you use the model.\n",
      "\n",
      "This zip contains 2 additional files to read the word2vec model with Python.\n",
      "The code for this was extracted from the Gensim Library which can be found here: https://radimrehurek.com/gensim/models/word2vec.html\n",
      "The only difference is that it does not use a strict encoding to read the model from the file.\n",
      "(One can easily integrate, inherit or extend the library or the Python files)"
     ]
    }
   ],
   "source": [
    "!cat /media/alex/data/word2vec_twitter_model/README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model2 = word2vecReader.Word2Vec.load_word2vec_format(\"/media/alex/data/word2vec_twitter_model/word2vec_twitter_model.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_model2[\"love\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_2(line):\n",
    "    v = np.zeros(40 * 400).reshape(40, 400)\n",
    "    words = line.strip().split(\" \")\n",
    "    _index = 0\n",
    "    for w in words:\n",
    "        if _index >= 40:\n",
    "            break\n",
    "        if w in wv_model2:\n",
    "            v[_index] = wv_model2[w]\n",
    "            _index += 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03832966,  0.00200333, -0.04462779, ...,  0.0044597 ,\n",
       "         0.02114136, -0.02238772],\n",
       "       [ 0.04128342,  0.07181156,  0.00676483, ..., -0.00052616,\n",
       "         0.01673744,  0.01691386],\n",
       "       [-0.01901733,  0.00757742,  0.00171131, ...,  0.05948232,\n",
       "         0.0217214 , -0.00660098],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_2(\"i love you so much\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1: torch.Size([10, 64, 38, 1])\n",
      "relu: torch.Size([10, 64, 38, 1])\n",
      "squeeze: torch.Size([10, 64, 38])\n",
      "max_pool1d: torch.Size([10, 64, 19])\n",
      "flat: torch.Size([10, 1216])\n",
      "f1: torch.Size([10, 64])\n",
      "f2: torch.Size([10, 32])\n",
      "f3: torch.Size([10, 1])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]], grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch=10, in_channels=2, height(num of tokens)=20, width(length of word2vec)=400\n",
    "_in = torch.randn(10, 2, 40, 400)\n",
    "\n",
    "# in_channels=2, out_channels=32, kernel_size=(3, 400)\n",
    "tmp_mod = torch.nn.Conv2d(2, 64, kernel_size=(3, 400), groups=2)\n",
    "\n",
    "_out = tmp_mod(_in)\n",
    "print(\"conv1:\", _out.size())\n",
    "\n",
    "_out = F.relu(_out)\n",
    "print(\"relu:\", _out.size())\n",
    "\n",
    "_out = torch.squeeze(_out)\n",
    "print(\"squeeze:\", _out.size())\n",
    "\n",
    "_out = F.max_pool1d(_out, 2)\n",
    "print(\"max_pool1d:\", _out.size())\n",
    "\n",
    "_out = _out.view(-1, 2 * 32 * 19)\n",
    "print(\"flat:\", _out.size())\n",
    "\n",
    "f1 = nn.Linear(1216, 64)\n",
    "_out = f1(_out)\n",
    "_out = F.relu(_out)\n",
    "print(\"f1:\", _out.size())\n",
    "\n",
    "f2 = nn.Linear(64, 32)\n",
    "_out = f2(_out)\n",
    "_out = F.relu(_out)\n",
    "print(\"f2:\", _out.size())\n",
    "\n",
    "f3 = nn.Linear(32, 1)\n",
    "_out = f3(_out)\n",
    "# _out = F.relu(_out)\n",
    "print(\"f3:\", _out.size())\n",
    "\n",
    "probs = F.softmax(_out, 1)\n",
    "print(probs)\n",
    "torch.max(probs, 1)[1]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CNNClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1ceb35123498>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNNClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CNNClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "_in = torch.randn(10, 2, 40, 400)\n",
    "model = CNNClassifier()\n",
    "model(_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train dataset\n",
    "\n",
    "import random\n",
    "\n",
    "with open(\"data/train_dataset.txt\", \"w\") as f:\n",
    "    lines = []\n",
    "    for line in open(\"data/0-train.txt\"):\n",
    "        lines.append(\"0\\t{}\".format(line))\n",
    "    for line in open(\"data/1-train.txt\"):\n",
    "        lines.append(\"1\\t{}\".format(line))\n",
    "    random.shuffle(lines)\n",
    "    \n",
    "    for line in lines:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.train_file = \"data/train_dataset.txt\"\n",
    "        self.train_batch_size = 128\n",
    "        \n",
    "        self.learning_rate = 0.001\n",
    "        self.window_size = 3\n",
    "        self.num_classes = 2\n",
    "        \n",
    "        self.num_epochs = 10\n",
    "        self.train_steps = None\n",
    "        \n",
    "        self.summary_interval = 100\n",
    "        \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "10 条信息，2 Input，40 词，每个词400向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading wv1 ...\n",
      "CPU times: user 1.28 s, sys: 1.28 s, total: 2.56 s\n",
      "Wall time: 4.52 s\n",
      "Loading wv2 ...\n",
      "CPU times: user 45.9 s, sys: 2.62 s, total: 48.5 s\n",
      "Wall time: 48.6 s\n"
     ]
    }
   ],
   "source": [
    "def read_wv1():\n",
    "    print(\"Loading wv1 ...\")\n",
    "    return Word2Vec.load(\"model/word2vec.mod\")\n",
    "        \n",
    "def read_wv2():\n",
    "    print(\"Loading wv2 ...\")\n",
    "    return word2vecReader.Word2Vec.load_word2vec_format(\n",
    "        \"/media/alex/data/word2vec_twitter_model/word2vec_twitter_model.bin\", binary=True)\n",
    "    \n",
    "%time _wv1 = read_wv1()\n",
    "%time _wv2 = read_wv2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.4 s, sys: 8 ms, total: 25.4 s\n",
      "Wall time: 25.4 s\n"
     ]
    }
   ],
   "source": [
    "# cnt = 0\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, filepath, batch_size):\n",
    "        self._file = open(filepath)\n",
    "        self._wv1 = _wv1\n",
    "        self._wv2 = _wv2\n",
    "        self._batch_size = batch_size\n",
    "        self._reset()\n",
    "        \n",
    "    def wv1(self, line):\n",
    "        v = np.zeros(40 * 400).reshape(40, 400)\n",
    "        words = line.strip().split(\" \")\n",
    "        _index = 0\n",
    "        for w in words:\n",
    "            if _index >= 40:\n",
    "                break\n",
    "            if w in self._wv1.wv:\n",
    "                v[_index] = self._wv1.wv[w]\n",
    "                _index += 1\n",
    "        return v\n",
    "\n",
    "    def wv2(self, line):\n",
    "            v = np.zeros(40 * 400).reshape(40, 400)\n",
    "            words = line.strip().split(\" \")\n",
    "            _index = 0\n",
    "            for w in words:\n",
    "                if _index >= 40:\n",
    "                    break\n",
    "                if w in self._wv2:\n",
    "                    v[_index] = self._wv2[w]\n",
    "                    _index += 1\n",
    "            return v\n",
    "\n",
    "    # 迭代时候每次先调用__iter__，初始化\n",
    "    # 接着调用__next__返回数据\n",
    "    # 如果没有buffer的时候，就补充数据_fill_buffer\n",
    "    # 如果buffer补充后仍然为空，则停止迭代\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._reset()\n",
    "        return self\n",
    "    \n",
    "    def _fill_buffer(self, size):\n",
    "        # global cnt\n",
    "\n",
    "        if self._buff_count == 0:\n",
    "            print(\"buffer空了，补充数据 ...\")\n",
    "            for line in self._file:\n",
    "                try:\n",
    "                    label, sentence = line.strip().split(\"\\t\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                label = int(label.strip())\n",
    "                sequence1 = self.wv1(sentence)\n",
    "                sequence2 = self.wv2(sentence)\n",
    "                self._buff_count += 1\n",
    "                self._buffer.append((label, [sequence1, sequence2]))\n",
    "                ## 直到满足size\n",
    "                \n",
    "                if self._buff_count >= size:\n",
    "                    break\n",
    "                    \n",
    "                # cnt += 1\n",
    "                # if cnt % 10000 == 0:\n",
    "                #    print(cnt)\n",
    "            self._buffer_iter = iter(self._buffer)\n",
    "    \n",
    "    def __next__(self):\n",
    "        self._fill_buffer(self._batch_size * 1000) # 每次读1024个batch作为buffer\n",
    "        \n",
    "        if self._buff_count == 0: # After filling, still empty, stop iter!\n",
    "            raise StopIteration\n",
    "                \n",
    "        # global cnt\n",
    "        label_batch = []\n",
    "        sequence_batch = []\n",
    "        for label, sequence in self._buffer_iter:\n",
    "            self._buff_count -= 1\n",
    "            label_batch.append(label)\n",
    "            sequence_batch.append(sequence)\n",
    "            if len(label_batch) == self._batch_size:\n",
    "                break\n",
    "\n",
    "\n",
    "        return {\"sequences\": np.array(sequence_batch), \"labels\": label_batch, }\n",
    "\n",
    "    def _reset(self):\n",
    "        self._file.seek(0)\n",
    "        self._buffer = []\n",
    "        self._buffer_iter = None\n",
    "        self._buff_count = 0\n",
    "        \n",
    "    def get_testdata(self):\n",
    "        labels = []\n",
    "        sequences = []\n",
    "        for line in open(\"data/0-test.txt\"):\n",
    "            labels.append(0)\n",
    "            sequences.append([self.wv1(line), self.wv2(line)])\n",
    "        for line in open(\"data/1-test.txt\"):\n",
    "            labels.append(1)\n",
    "            sequences.append([self.wv1(line), self.wv2(line)])\n",
    "        return torch.LongTensor(labels), torch.Tensor(sequences)\n",
    "\n",
    "    \n",
    "train_set = Dataset(config.train_file, config.train_batch_size)\n",
    "%time test_labels, test_X = train_set.get_testdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer空了，补充数据 ...\n",
      "0 torch.Size([128, 2, 40, 400]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_set):\n",
    "    if i % 500 == 0:\n",
    "        sequences = torch.Tensor(batch[\"sequences\"])\n",
    "        labels = torch.LongTensor(batch[\"labels\"])\n",
    "        print(i, sequences.size(), labels.size())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "\n",
    "        # 2 in- channels, 32 out- channels, 3 * 400 windows size\n",
    "        self.conv = torch.nn.Conv2d(2, 64, kernel_size=(3, 400), groups=2) \n",
    "        self.f1 = nn.Linear(1216, 64)\n",
    "        self.f2 = nn.Linear(64, 32)\n",
    "        self.f3 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = F.relu(out)\n",
    "        out = torch.squeeze(out)\n",
    "        out = F.max_pool1d(out, 2)\n",
    "        out = out.view(-1, 2 * 32 * 19) # 9 is after pooling\n",
    "        out = F.relu(self.f1(out))\n",
    "        out = F.relu(self.f2(out))\n",
    "        out = self.f3(out)\n",
    "        # print(out.size())\n",
    "        \n",
    "        probs = F.softmax(out, dim=1)\n",
    "        # print(probs)\n",
    "        classes = torch.max(probs, 1)[1]\n",
    "\n",
    "        return probs, classes\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO\t2018-11-26 19:14:19,993\t==================== Epoch: 1 ====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer空了，补充数据 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO\t2018-11-26 19:14:29,585\tstep = 0, loss = 0.7093743681907654\n",
      "INFO\t2018-11-26 19:14:37,474\tstep = 100, loss = 0.6470441317558289\n",
      "INFO\t2018-11-26 19:14:43,797\tstep = 200, loss = 0.6051749521493912\n",
      "INFO\t2018-11-26 19:14:50,283\tstep = 300, loss = 0.5732173711061478\n",
      "INFO\t2018-11-26 19:14:56,350\tstep = 400, loss = 0.558872994184494\n",
      "INFO\t2018-11-26 19:15:03,328\tstep = 500, loss = 0.5485111489892006\n",
      "INFO\t2018-11-26 19:15:09,534\tstep = 600, loss = 0.5401800882816314\n",
      "INFO\t2018-11-26 19:15:16,229\tstep = 700, loss = 0.5370687690377235\n",
      "INFO\t2018-11-26 19:15:22,392\tstep = 800, loss = 0.5362289342284202\n",
      "INFO\t2018-11-26 19:15:28,223\tstep = 900, loss = 0.5305288290977478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer空了，补充数据 ...\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format=\"%(levelname)s\\t%(asctime)s\\t%(message)s\", level=logging.INFO)\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def train():\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    writer = SummaryWriter(log_dir=\"log\")\n",
    "    \n",
    "    epoch = 0\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(1, config.num_epochs + 1):\n",
    "        logging.info(\"==================== Epoch: {} ====================\".format(epoch))\n",
    "        running_losses = []\n",
    "        for batch in train_set:\n",
    "\n",
    "            sequences = torch.Tensor(batch[\"sequences\"])\n",
    "            labels = torch.LongTensor(batch[\"labels\"])\n",
    "\n",
    "            # Predict\n",
    "            try:\n",
    "                probs, classes = model(sequences)\n",
    "            except:\n",
    "                print(sequences.size(), labels.size())\n",
    "                exit(-1)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            losses = loss_function(probs, labels)\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log summary\n",
    "            running_losses.append(losses.data.item())\n",
    "            if step % config.summary_interval == 0:\n",
    "                loss = sum(running_losses) / len(running_losses)\n",
    "                writer.add_scalar(\"train/loss\", loss, step)\n",
    "                logging.info(\"step = {}, loss = {}\".format(step, loss))\n",
    "                running_losses = []\n",
    "            \n",
    "            step += 1\n",
    "        \n",
    "        # Classification report\n",
    "        probs, y_pred = model(test_X)\n",
    "        target_names = ['pro-hillary', 'pro-trump']\n",
    "        print(classification_report(test_labels, y_pred, target_names=target_names))\n",
    "\n",
    "        epoch += 1\n",
    "        \n",
    "model = CNNClassifier()\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
